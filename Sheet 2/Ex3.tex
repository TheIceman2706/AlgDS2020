\section*{Exercise 3}
Using a description scheme where for each node is encoded using the binary representation of the index of the splitting feature using $\Sigma = \lbrace0,1\rbrace$ and for each node $\Sigma^{\text{No. of features}}$
A 16 node decisiontree with 6 features uses $16\cdot 2^3$ bits hence \[n=16=2^3=128.\]

We know from \[m \ge \frac{1}{\epsilon}\left(n \ln |\Sigma| + 2\ln\left(\frac{2}{\delta}\right)\right)\] that for $\epsilon = 0.05$ and $\delta = 0.2$ we need at least $m\ge1821$ trianing examples to archive the described accuracy and propability for the described tree.
Respecting the lower boud for $m$ we can set $m\le 1821$ as upper bound because 1821 training exaples surfice. Hence we don't need more than these samples.